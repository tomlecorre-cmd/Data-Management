########################################################################################
#                ETAPE 1 : RECUPERATION DES DONNEES AVEC API YAHOO
 
########################################################################################

# pip install yfinance
import os

# Crée le dossier 'data' s'il n'existe pas déjà
os.makedirs("data", exist_ok=True)
# Les actions
import yfinance as yf

symboles = "MC.PA TTE.PA OR.PA RMS.PA SAN.PA AIR.PA SU.PA BNP.PA GLE.PA ACA.PA CS.PA DG.PA VIE.PA ENGI.PA ORA.PA CAP.PA STMPA.PA SAF.PA HO.PA MT.AS RNO.PA ML.PA PUB.PA BN.PA CA.PA KER.PA LR.PA SGO.PA AI.PA EN.PA URW.PA WLN.PA VIV.PA TEP.PA DSY.PA"

print("Téléchargement en cours")

# Téléchargement groupé
data = yf.download(symboles, period="max", group_by='ticker', auto_adjust=False, progress=False, threads=True)

# Transformation en liste (Stacking) pour avoir les 200 000+ lignes
df_final = data.stack(level=0).reset_index()

# Renommage de la colonne des symboles
df_final.rename(columns={'level_1': 'Ticker'}, inplace=True)

# Vérification finale
print(f"Il y a  {len(df_final)} lignes.")
print(f"Colonnes : {df_final.columns.tolist()}")

# Sauvegarde
df_final.to_csv("data/cac40.csv", index=False)


# Les macros
import yfinance as yf
import pandas as pd


symboles_macro = "BZ=F NG=F GC=F SI=F HG=F EURUSD=X EURJPY=X EURGBP=X ^TNX ^VIX ^NDX BTC-USD"

print(f"Téléchargement de {len(symboles_macro.split())} indicateurs MACRO ")

# 1. Téléchargement groupé
data_macro = yf.download(symboles_macro, period="max", group_by='ticker', auto_adjust=False, progress=False, threads=True)

# 2. Transformation en liste (Stacking)
df_macro = data_macro.stack(level=0).reset_index()

# 3. Renommage propre
df_macro.rename(columns={'level_1': 'Ticker'}, inplace=True)

# 4. Vérification
print(f"Macro téléchargée : {len(df_macro)} lignes.")
print("Indicateurs récupérés :")
print(df_macro['Ticker'].unique())

# 5. Sauvegarde
df_macro.to_csv("data/macro.csv", index=False)


"""                             EXPLICATION:

Afin de répondre aux exigences de volume du projet (> 200 000 observations), nous avons 
automatisé la constitution de notre jeu via le langage Python.
Plutôt que de télécharger manuellement des dizaines de fichiers, nous avons développé un 
script utilisant la librairie yfinance, qui intéragit avec l'API publique de Yahoo Finance.

Notre Démarche s'est déroulée en trois points clés :
- Ciblage : Nous avons défini un panier d'actifs représentatif de l'économie française,
composé des principales capitalisation du CAC40 (LVMH, TotalEnergies, BNP Paribas, etc...).
- Extraction Massive : Le script a récupéré l'histoire complet de cotation (Ouversture, Plus
haut, Plus bas, Clôture, Volume) pour chaque entreprise.
- Structuration : Les données, initialement fragmentées, ont été fusionnées et pivotées
(technique de stacking) pour obtenir un fichier CSV unique et structuré.

Ce processus nous a permis de générer instantanément une base de données brute de plus de 
200 000 lignes, garantissant ainsi la robustesse statistique nécessaire pour nos futures 
analyses.
"""


########################################################################################
#                       ETAPE 2 : NETTOYAGE DES DONNEES
 
########################################################################################

import pandas as pd
import numpy as np


# On importe nos données dans un dataframe
df = pd.read_csv("data/cac40.csv")

df_macro = pd.read_csv("data/macro.csv")

# on verifie le nbr de ligne
print(df.shape)
df_macro.shape


# on regarde le type de chaque objet 
print(df.dtypes)
df_macro.dtypes

# on va changer le type de la date 
df['Date'] = pd.to_datetime(df['Date'])
print(df.dtypes)
df_macro['Date'] = pd.to_datetime(df_macro['Date'])
print(df_macro.dtypes)

"""                              EXPLICATION: 
1) Nous avons converti explicitement cette colonne au format datetime6 à l'aide de la 
fonction pd.to_datetime().

Intérêt pour le projet: Cette conversion est un prérequis indispensable pour trois raisons :
- Filtrage temporel: Elle nous permet d'appliquer des opérateurs mathématiqus sur le temps (
ex: sélectionner uniquement les dates > 2000-01-01), ce qui est impossible sur du texte brut.
- Tri chronologique: Elle garantit que les observations sont bien ordonnées du passé vers le
futur, ce qui est crucial pour le calcul des rendements (variation par rapport à la veille).
- Visualisation: Elle permet aux bibliothèques graphiques (comme celles utilisées dans Streamlit) 
de reconnaître l'axe des abscisses comme échelle de temps continue et non comme des catégories 
distinctes.

2) Nous avons appliqué un filtre pour exclure toutes les observations antérieues au 1er Janvier 
2000.

Justification économique et technique: Cette réduction du périmètre historique repose sur les 
trois arguments majeures:
- Homogénéité monétaire (L'effet Euro): L'analyse de séries temporelles financières sur une 
longue période est biaisée par les changements de devises. En nous concentrant sur la période 
post-2000, nous travaillons sur une ère économique cohérente marquée par l'adaption de l'Euro 
'introduit sur les marchés financier en 1999), évitant ainsi les artefacts liés au taux de 
conversion Franc/Euro des années 80-90.
- Disponibilité des données (Data Quality): La composition du CAC40 a fortement évolué. De 
nombreuses entreprises technologiques ou industrielles actuelles n'étaient pas cotées ou 
n'existaient pas sous leur forme actuelle dans les années 1980. Démarrer en 2000 permet de 
maximiser le nombre d'entreprises ayant un historique complet et de réduire le nombre de 
valeurs manquantes (NaN) en début de jeu de données.
- Pertinence des cycles boursiers: La période 2000-2025 couvre les cycles économiques modernes 
les plus pertinents pour votre analyse: l'éclatement de la bulle Internent (2000), la crise 
des Subprimes (2008), la crise de la dette souveraine (2011) et le chox du COVID-19 (2020).

"""

df = df[df['Date'] >= '2000-01-01']
df_macro = df_macro[df_macro['Date']>='2000-01-01']


print(f"Il y a {df.isna().sum().sum()} valeurs manquantes dans les actions")
print(f"Il y a {df.duplicated().sum()} valeurs en double dans les actions")
print(f"Il y a {df.isna().sum().sum()} valeurs manquantes dans les macros")
print(f"Il y a {df.duplicated().sum()} dans les macros")



########################################################################################
#                           ETAPE 3 : CREATION DE VARIABLES
 
########################################################################################


# 1 Rentabilité
df["Rentabilite"] = df.groupby('Ticker')['Close'].pct_change()*100


# Forcément on aura des na pour toutes le jour 2001-01-01 donc on remplace par un 0 car pas de données
#df['Rentabilité']  = df["Rentabilité"].fillna(0)
# je crois pas le faire mtn pcq ça va bz les stats sur  la volatilité sinon 
print(f"il y a {df.isna().sum().sum()} valeurs manquantes")


# On groupe par Ticker pour ne pas mélanger les entreprises
# On prend 30 jours pour avoir une vraie mesure du risque
df['Volatilite_30j'] = df.groupby('Ticker')['Rentabilite'].transform(lambda x: x.rolling(window=30).std())


print(f"il y a {df.isna().sum().sum()} valeurs manquantes")



df.dropna(inplace=True)

print(f"Nombre de valeurs manquantes restantes : {df.isna().sum().sum()}")


df.to_csv("data/cac40_final.csv", index=False)
df_macro.to_csv('data/macros_final.csv',index=False)
print(" Fichier sauvegardé ")


df_cac = pd.read_csv('data/cac40_final.csv')
df_macros = pd.read_csv("data/macros_final.csv")

df_final = pd.merge(df_cac, df_macros, on='Date',how='left')

df_final.tail()


df_final.to_csv("data/data_final.csv", index=False)



""""                                    EXPLICATION:

Dans le but d'enrichir notre analyse et de dépasser la simple observation des prix bruts, nous 
avons génére deux indicateurs financiers fondamentaux:
- La Rentabilité Journalière: Les cours de bourse bruts sont difficilement comparables entre 
eux (ex: une action LVMH vaut environ 700 euros alors qu'une action Orange vaut environ 10 euros).
En calculant la variation en pourcentage par rapport à la veille (Daily Return), nous créons 
une métrique standardisée. Cela nous permet de comparer directement la performance de 
différentes entreprises, quelle que soit elur valorisation initiale, et d'identifer les jours 
de forte croissance ou de décroissance.
- La volatilité sur 30 jours (Volatilite_30j) Méthodologie: Ecart-type (Standard Deviation) 
des rentabilités calculé sur une fenêtre glissante (rolling window) de 30 jours.

Justification : La rentabilité sele ne suffit pas à évaluer un investissement: il est crucial 
de mesurer le risque associé. Cette variable nous permet de quantifier l'instabilité du titre. 
Une volatilité élevée indique des variations de prix brutales (incertitude forte), tandis 
qu'une volatilité faible signale une valeur stable. Le choix d'une fenêtre de 30 jours permet 
d'observer l'évolution du risquemois par mois, lissant des bruits journaliers tout en restant 
réactif aux changements de tendance de marché.

"""

